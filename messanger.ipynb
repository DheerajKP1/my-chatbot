{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important libraries\n",
    "import streamlit as st\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from typing import List, Literal, Optional\n",
    "import chromadb\n",
    "import uuid\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import re\n",
    "import tiktoken\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(\n",
    "    temperature=0,\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name=\"deepseek-r1-distill-llama-70b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nHello! How can I assist you today? ðŸ˜Š'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = llm.invoke(\"Hi\")\n",
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dheeraj\\AppData\\Local\\Temp\\ipykernel_1928\\791078931.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant. Use the provided context to answer Dheeraj, 2024-03-20 questions accurately.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the weather like today in New Delhi?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Use the provided context to answer {name}, {date} questions accurately.\"),\n",
    "        (\"placeholder\", \"{query_with_context}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fill the template with actual values\n",
    "prmp = prompt.invoke({\n",
    "    \"name\": \"Dheeraj\",\n",
    "    \"date\": \"2024-03-20\",\n",
    "    \"query_with_context\": [(\"What is the weather like today in New Delhi?\")]\n",
    "})\n",
    "\n",
    "# Print the formatted prompt\n",
    "print(prmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token', '##ization', 'is', 'important', 'in', 'nl', '##p', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Tokenization is important in NLP!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 3419, 102]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\"TOM\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 19204,  3989,  2003,  2590,  1999, 17953,  2361,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.5625, -0.2248,  0.2079,  ..., -0.1317,  0.5404, -0.2481],\n",
       "         [-0.2984, -0.0176,  0.0511,  ..., -0.3413,  1.1455, -1.1604],\n",
       "         [-0.4177, -0.2039,  0.3929,  ..., -0.2503,  0.9900,  0.0939],\n",
       "         ...,\n",
       "         [-0.9699, -0.3478,  0.2634,  ...,  0.1652,  0.7106, -0.2993],\n",
       "         [-0.5622, -0.0389,  0.2668,  ..., -0.5186,  1.0669, -0.4123],\n",
       "         [-0.5625, -0.2248,  0.2079,  ..., -0.1317,  0.5404, -0.2481]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.1013, -0.0475,  0.1447, -0.0351, -0.1429, -0.0728, -0.0626,  0.1253,\n",
       "          0.0631, -0.0427,  0.0023,  0.0215, -0.0063, -0.0172, -0.0513,  0.1690,\n",
       "          0.0357,  0.0085,  0.1014,  0.1401, -0.0222,  0.0570,  0.1195, -0.1327,\n",
       "         -0.0566,  0.0637, -0.0355,  0.0064, -0.0809,  0.0585,  0.0251,  0.0185,\n",
       "         -0.1385,  0.0537,  0.0637, -0.0616, -0.0306, -0.0131, -0.0147,  0.1220,\n",
       "         -0.0863, -0.0379,  0.0493, -0.0687, -0.0316, -0.0214, -0.0484, -0.0666,\n",
       "         -0.0211,  0.0017, -0.1620, -0.0575,  0.1125,  0.0157, -0.0198, -0.1133,\n",
       "         -0.0162,  0.0116, -0.1377,  0.0520, -0.0040,  0.0196, -0.0279, -0.0991,\n",
       "          0.0412,  0.0658, -0.1078, -0.1681, -0.0308, -0.1018, -0.0202, -0.1067,\n",
       "          0.0381,  0.0870,  0.0374,  0.0624, -0.0222, -0.0559, -0.1042,  0.0172,\n",
       "          0.0763, -0.0751, -0.0258, -0.1166,  0.0363,  0.0333,  0.1609,  0.0694,\n",
       "          0.0413, -0.0153,  0.0099, -0.0515,  0.0003, -0.0477,  0.1413,  0.0123,\n",
       "          0.0105, -0.0322, -0.0596,  0.0245,  0.0110, -0.0703, -0.0362,  0.0913,\n",
       "          0.0955, -0.1100, -0.0536,  0.0340,  0.1704, -0.0098,  0.0950,  0.0929,\n",
       "          0.0869, -0.0551, -0.0355, -0.1257, -0.0177, -0.0756,  0.0586,  0.0098,\n",
       "          0.1835, -0.0455,  0.0954, -0.0052, -0.0399,  0.0348, -0.0124, -0.0646,\n",
       "         -0.1842,  0.0207,  0.0693, -0.0800, -0.0621,  0.0867, -0.1023, -0.0109,\n",
       "         -0.0082,  0.0008,  0.0229,  0.1542,  0.0233, -0.2498,  0.1397, -0.1369,\n",
       "          0.1471, -0.1614, -0.0971,  0.1072, -0.1616,  0.0695,  0.1798, -0.0792,\n",
       "         -0.1044, -0.0261, -0.1105, -0.0567, -0.1014, -0.0659,  0.0102, -0.0759,\n",
       "         -0.0935,  0.0054, -0.0538,  0.0873, -0.1200,  0.0957,  0.0221, -0.0264,\n",
       "          0.0987, -0.0021, -0.0510, -0.0374, -0.0616, -0.1353,  0.0411,  0.0540,\n",
       "          0.0468,  0.0440, -0.0242,  0.0052,  0.0988,  0.0029, -0.0695, -0.0654,\n",
       "         -0.0163,  0.0399,  0.0354,  0.1585, -0.0984, -0.0931,  0.0809,  0.1409,\n",
       "         -0.0559,  0.1073,  0.0425, -0.0915,  0.0719, -0.0065, -0.0617, -0.0017,\n",
       "          0.0261,  0.0708, -0.0969, -0.0318,  0.0294,  0.0150, -0.0899, -0.0572,\n",
       "          0.0568,  0.1292,  0.0281,  0.1322, -0.0764, -0.0977,  0.0146, -0.1326,\n",
       "          0.0378,  0.0067,  0.0864, -0.0414, -0.0161, -0.0206, -0.0596,  0.0523,\n",
       "         -0.0180, -0.0973,  0.0785,  0.0096,  0.0052,  0.0236, -0.0220,  0.0430,\n",
       "         -0.1419,  0.0207,  0.0489, -0.0369,  0.1125, -0.1600,  0.1037,  0.1884,\n",
       "         -0.0298, -0.0130, -0.1109,  0.0754, -0.0557, -0.0601, -0.0787, -0.0752,\n",
       "         -0.0094, -0.0202,  0.1325,  0.0918,  0.0980,  0.0040, -0.0267, -0.0124,\n",
       "         -0.0229,  0.0969,  0.0437,  0.0468,  0.0903,  0.0101, -0.0309,  0.1001,\n",
       "          0.0061,  0.0179, -0.0288, -0.0586,  0.0579, -0.0230,  0.0321, -0.0345,\n",
       "         -0.0453,  0.1257, -0.0131, -0.0516, -0.0196, -0.0647, -0.0910,  0.0341,\n",
       "         -0.0998,  0.0025,  0.0242,  0.0461, -0.0012,  0.1617, -0.0708, -0.1209,\n",
       "         -0.0779,  0.1455, -0.0616, -0.0067, -0.0521,  0.1653, -0.1119,  0.1133,\n",
       "         -0.1034,  0.0288,  0.0345, -0.0300, -0.0467, -0.0376,  0.0661,  0.0853,\n",
       "         -0.0642,  0.2397, -0.0817, -0.1152,  0.0550,  0.0363,  0.0444, -0.0072,\n",
       "         -0.0816,  0.0048, -0.0536, -0.0033,  0.1339, -0.0267, -0.1096, -0.0312,\n",
       "         -0.0548, -0.0658,  0.0762, -0.0641,  0.0892, -0.0500,  0.0890,  0.0183,\n",
       "         -0.1426,  0.0868,  0.0565,  0.0411, -0.0035, -0.0007,  0.0212, -0.1640,\n",
       "         -0.0287,  0.1362,  0.0241,  0.1242,  0.0485, -0.0637, -0.0606, -0.1120,\n",
       "         -0.0063, -0.0793,  0.0556,  0.0803,  0.0086,  0.0185, -0.0455, -0.0683,\n",
       "         -0.0577,  0.0064, -0.0026, -0.0729, -0.1040,  0.0038,  0.0212,  0.0501,\n",
       "         -0.0913, -0.0845, -0.1401, -0.0488,  0.0583,  0.0907,  0.0119, -0.0744,\n",
       "          0.0541, -0.0707, -0.1248, -0.0414,  0.0705, -0.0183,  0.0490, -0.0966,\n",
       "         -0.0303,  0.0303, -0.0784, -0.0522, -0.1081,  0.0097, -0.0428, -0.0823]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are a helpful AI bot. Your name is Carl.\\nHuman: Who are you?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is Carl.\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "prompt_value = template.format(user_input=\"Who are you?\")\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I\\'m trying to figure out how to respond to the user\\'s question. They asked, \"Who are you?\" and the system provided a response where the AI introduces itself as Carl, a helpful AI bot. Now, I need to think through how to approach this.\\n\\nFirst, I should consider the user\\'s intent. They\\'re asking for identification, so the response should clearly state what the AI is. The system\\'s response does that by saying, \"I\\'m Carl, a helpful AI bot...\" which is good. It\\'s concise and informative.\\n\\nI wonder if there\\'s anything else the user might want to know. Maybe they\\'re curious about the AI\\'s capabilities or limitations. The response mentions assisting with questions and providing information, which covers that. It also reassures the user that the AI is here to help, which is positive.\\n\\nIs there a way to make the response more engaging? Perhaps by adding a friendly emoji or a question to prompt further interaction. However, the current response is straightforward and professional, which might be more appropriate depending on the context.\\n\\nI should also think about the structure. The response starts with a greeting, introduces the AI\\'s name and purpose, and then offers assistance. That\\'s a logical flow. Maybe adding a prompt like \"How can I assist you today?\" could encourage the user to continue the conversation.\\n\\nAnother consideration is the tone. The response is polite and helpful, which is exactly what you\\'d want from a customer service AI. It\\'s not too casual, which might be better for maintaining professionalism.\\n\\nI should check if the response addresses all parts of the user\\'s question. The user asked, \"Who are you?\" and the response answers that by providing the name and purpose. It doesn\\'t go into unnecessary details, which keeps it focused.\\n\\nPerhaps the response could mention specific areas of expertise or examples of tasks the AI can help with. That might give the user a better idea of what to ask next. For example, mentioning things like answering questions, providing information, helping with problems, etc.\\n\\nWait, the response does say \"whether it\\'s answering questions, providing information, or helping with a problem.\" So that covers it. It\\'s comprehensive without being overwhelming.\\n\\nI think the response is effective as it is. It\\'s clear, concise, and covers the necessary information. The user knows exactly what to expect from the AI, which is important for building trust and clarity.\\n\\nMaybe the only thing to consider is adding a bit more warmth or a welcoming phrase to make the interaction feel more personable. But that depends on the intended use case. If it\\'s for a professional setting, the current tone is appropriate.\\n\\nIn summary, the response effectively introduces the AI, states its purpose, and offers assistance. It\\'s well-structured and covers all the bases the user might be interested in. There\\'s not much else needed unless specific additional information is required.\\n</think>\\n\\nHello! I\\'m Carl, a helpful AI bot here to assist you. Whether it\\'s answering questions, providing information, or helping with a problem, I\\'m ready to lend a hand. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 634, 'prompt_tokens': 23, 'total_tokens': 657, 'completion_time': 2.305454545, 'prompt_time': 0.005200185, 'queue_time': 0.246066934, 'total_time': 2.31065473}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_454c494f52', 'finish_reason': 'stop', 'logprobs': None}, id='run-776a57e0-d066-4463-8f76-65b2a7e31ba5-0', usage_metadata={'input_tokens': 23, 'output_tokens': 634, 'total_tokens': 657})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=llm.invoke(prompt_value)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for generating Embedding\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_vector_store = InMemoryVectorStore(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_id(config: RunnableConfig) -> str:\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"User ID needs to be provided to save a memory.\")\n",
    "\n",
    "    return user_id\n",
    "\n",
    "@tool\n",
    "def save_recall_memory(memory: str, config: RunnableConfig) -> str:\n",
    "    \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    document = Document(\n",
    "        page_content=memory, id=str(uuid.uuid4()), metadata={\"user_id\": user_id}\n",
    "    )\n",
    "    recall_vector_store.add_documents([document])\n",
    "    return memory\n",
    "\n",
    "@tool\n",
    "def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:\n",
    "    \"\"\"Search for relevant memories.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "\n",
    "    def _filter_function(doc: Document) -> bool:\n",
    "        return doc.metadata.get(\"user_id\") == user_id\n",
    "\n",
    "    documents = recall_vector_store.similarity_search(\n",
    "        query, k=3, filter=_filter_function\n",
    "    )\n",
    "    return [document.page_content for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [save_recall_memory, search_recall_memories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    # add memories that will be retrieved based on the conversation context\n",
    "    recall_memories: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for the agent\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant with advanced long-term memory\"\n",
    "            \" capabilities. Powered by a stateless LLM, you must rely on\"\n",
    "            \" external memory to store information between conversations.\"\n",
    "            \" Utilize the available memory tools to store and retrieve\"\n",
    "            \" important details that will help you better attend to the user's\"\n",
    "            \" needs and understand their context.\\n\\n\"\n",
    "            ## Rag Prompt\n",
    "            \" You are designed to help user in finding the best possible answer from uploased pdf\"\n",
    "            \" files. You can ask user for clarification if you are unsure about\"\n",
    "            \" Use the following context while answering the user's question:\\n\\n\"\n",
    "            \" Context :\\n {context}\\n\\n\"\n",
    "            ##########\n",
    "            \"Memory Usage Guidelines:\\n\"\n",
    "            \"1. Actively use memory tools (save_core_memory, save_recall_memory)\"\n",
    "            \" to build a comprehensive understanding of the user.\\n\"\n",
    "            \"2. Make informed suppositions and extrapolations based on stored\"\n",
    "            \" memories.\\n\"\n",
    "            \"3. Regularly reflect on past interactions to identify patterns and\"\n",
    "            \" preferences.\\n\"\n",
    "            \"4. Update your mental model of the user with each new piece of\"\n",
    "            \" information.\\n\"\n",
    "            \"5. Cross-reference new information with existing memories for\"\n",
    "            \" consistency.\\n\"\n",
    "            \"6. Prioritize storing emotional context and personal values\"\n",
    "            \" alongside facts.\\n\"\n",
    "            \"7. Use memory to anticipate needs and tailor responses to the\"\n",
    "            \" user's style.\\n\"\n",
    "            \"8. Recognize and acknowledge changes in the user's situation or\"\n",
    "            \" perspectives over time.\\n\"\n",
    "            \"9. Leverage memories to provide personalized examples and\"\n",
    "            \" analogies.\\n\"\n",
    "            \"10. Recall past challenges or successes to inform current\"\n",
    "            \" problem-solving.\\n\\n\"\n",
    "            \"## Recall Memories\\n\"\n",
    "            \"Recall memories are contextually retrieved based on the current\"\n",
    "            \" conversation:\\n{recall_memories}\\n\\n\"\n",
    "            \"## Instructions\\n\"\n",
    "            \"Engage with the user naturally, as a trusted colleague or friend.\"\n",
    "            \" There's no need to explicitly mention your memory capabilities.\"\n",
    "            \" Instead, seamlessly incorporate your understanding of the user\"\n",
    "            \" into your responses. Be attentive to subtle cues and underlying\"\n",
    "            \" emotions. Adapt your communication style to match the user's\"\n",
    "            \" preferences and current emotional state. Use tools to persist\"\n",
    "            \" information you want to retain in the next conversation. If you\"\n",
    "            \" do call tools, all text preceding the tool call is an internal\"\n",
    "            \" message. Respond AFTER calling the tool, once you have\"\n",
    "            \" confirmation that the tool completed successfully.\\n\\n\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def agent(state: State) -> State:\n",
    "    \"\"\"Process the current state and generate a response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        schemas.State: The updated state with the agent's response.\n",
    "    \"\"\"\n",
    "    bound = prompt | llm\n",
    "    recall_str = (\n",
    "        \"<recall_memory>\\n\" + \"\\n\".join(state[\"recall_memories\"]) + \"\\n</recall_memory>\"\n",
    "    )\n",
    "    # Load the vector store\n",
    "    print(state[\"messages\"][-1].content)\n",
    "    vectorstore = Chroma(persist_directory=\"chroma-BAAI\", embedding_function=embedding_model)\n",
    "    res = vectorstore.similarity_search(state[\"messages\"][-1].content, k=3) \n",
    "    context = \" \".join([doc.page_content for doc in res])\n",
    "\n",
    "    prediction = bound.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"context\": context,\n",
    "            \"recall_memories\": recall_str\n",
    "            \n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": [prediction],\n",
    "    }\n",
    "\n",
    "\n",
    "def load_memories(state: State, config: RunnableConfig) -> State:\n",
    "    \"\"\"Load memories for the current conversation.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "\n",
    "    Returns:\n",
    "        State: The updated state with loaded memories.\n",
    "    \"\"\"\n",
    "    convo_str = get_buffer_string(state[\"messages\"])\n",
    "    convo_str = tokenizer.decode(tokenizer.encode(convo_str)[:2048])\n",
    "    recall_memories = search_recall_memories.invoke(convo_str, config)\n",
    "    return {\n",
    "        \"recall_memories\": recall_memories,\n",
    "    }\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    \"\"\"Determine whether to use tools or end the conversation based on the last message.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"tools\", \"__end__\"]: The next step in the graph.\n",
    "    \"\"\"\n",
    "    msg = state[\"messages\"][-1]\n",
    "    if msg.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph and add nodes\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(load_memories)\n",
    "builder.add_node(agent)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Add edges to the graph\n",
    "builder.add_edge(START, \"load_memories\")\n",
    "builder.add_edge(\"load_memories\", \"agent\")\n",
    "builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END])\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_stream_chunk(chunk):\n",
    "    for node, updates in chunk.items():\n",
    "        print(f\"Update from node: {node}\")\n",
    "        if \"messages\" in updates:\n",
    "            updates[\"messages\"][-1].pretty_print()\n",
    "        else:\n",
    "            print(updates)\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'load_memories': {'recall_memories': []}}\n",
      "Hey I am dheeraj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dheeraj\\AppData\\Local\\Temp\\ipykernel_1928\\3863353399.py:16: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"chroma-BAAI\", embedding_function=embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'agent': {'messages': [AIMessage(content='<think>\\nOkay, so the user just said, \"Hey I am dheeraj.\" I need to respond in a friendly and welcoming manner. Since I don\\'t have any prior memory of Dheeraj, I should introduce myself and offer help. I\\'ll make sure to keep the tone warm and approachable, maybe ask how I can assist him today. Also, I should remember to save this interaction in the memory tools so I can refer back to it in future conversations. That way, I can provide more personalized support next time he reaches out.\\n</think>\\n\\nHey Dheeraj! Welcome. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 407, 'total_tokens': 537, 'completion_time': 0.472727273, 'prompt_time': 0.072329353, 'queue_time': 0.046950924000000005, 'total_time': 0.545056626}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_2834edf0f4', 'finish_reason': 'stop', 'logprobs': None}, id='run-ca866f86-a507-4b22-b917-869223513562-0', usage_metadata={'input_tokens': 407, 'output_tokens': 130, 'total_tokens': 537})]}}\n"
     ]
    }
   ],
   "source": [
    "# NOTE: we're specifying `user_id` to save memories for a given user\n",
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}\n",
    "i =0\n",
    "for chunk in graph.stream({\"messages\": [(\"user\", \"Hey I am dheeraj\")]}, config=config):\n",
    "    print (i)\n",
    "    i=i+1\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is WHO\u001b[39m\u001b[38;5;124m\"\u001b[39m)]}, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "response = graph.stream({\"messages\": [HumanMessage(content=\"What is WHO\")]}, config=config)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey I am dheeraj\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am dheeraj\n",
      "{'agent': {'messages': [AIMessage(content='<think>\\nAlright, the user has just said, \"I am dheeraj.\" Looking back at the conversation history, I see that they introduced themselves as Dheeraj earlier. So, this might be a way of reaffirming their name or perhaps they\\'re starting a new topic.\\n\\nI should acknowledge their introduction warmly to make them feel welcome. Since they\\'ve mentioned their name again, maybe they want to share more about themselves or need assistance with something specific.\\n\\nI\\'ll respond by greeting them and asking how I can assist. Keeping the tone friendly and open-ended should encourage them to share more details about what they need help with.\\n</think>\\n\\nHello, Dheeraj! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 1404, 'total_tokens': 1549, 'completion_time': 0.527272727, 'prompt_time': 0.075106812, 'queue_time': 0.207020191, 'total_time': 0.602379539}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_454c494f52', 'finish_reason': 'stop', 'logprobs': None}, id='run-c55ae800-d31c-4157-aa62-6333bc4a4f94-0', usage_metadata={'input_tokens': 1404, 'output_tokens': 145, 'total_tokens': 1549})]}}\n"
     ]
    }
   ],
   "source": [
    "output = list(graph.stream({\"messages\": [HumanMessage(content=\"I am dheeraj\")]}, config=config))\n",
    "print(output[-1])  # Now you can safely access the first element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, the user has just said, \"I am dheeraj.\" Looking back at the conversation history, I see that they introduced themselves as Dheeraj earlier. So, this might be a way of reaffirming their name or perhaps they\\'re starting a new topic.\\n\\nI should acknowledge their introduction warmly to make them feel welcome. Since they\\'ve mentioned their name again, maybe they want to share more about themselves or need assistance with something specific.\\n\\nI\\'ll respond by greeting them and asking how I can assist. Keeping the tone friendly and open-ended should encourage them to share more details about what they need help with.\\n</think>\\n\\nHello, Dheeraj! How can I assist you today?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1]['agent']['messages'][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': []}\n",
      "\n",
      "\n",
      "What is WHO\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "\n",
      "The **World Health Organization (WHO)** is a specialized agency of the United Nations responsible for international public health. It was established on July 22, 1946, and is headquartered in Geneva, Switzerland. The WHO's main objective is to promote health, keep the world safe, and serve all people. It works to achieve its goals through various programs, including global health security, non-communicable diseases, mental health, and promoting health equity.\n",
      "\n",
      "The WHO is known for its role in setting global health policies, coordinating responses to infectious disease outbreaks (e.g., COVID-19, Ebola), and providing technical assistance to countries to improve their healthcare systems. It also plays a key role in promoting vaccination, eradicating diseases (e.g., smallpox), and addressing global health challenges like malaria, tuberculosis, and HIV/AIDS.\n",
      "\n",
      "Would you like me to provide more details about a specific aspect of the WHO? \n",
      "\n",
      "<save_recall_memory>\n",
      "{\n",
      "  \"user_interest\": \"WHO\",\n",
      "  \"interaction_date\": \"2023-11-10\",\n",
      "  \"context\": \"User asked about the World Health Organization (WHO). Provided a brief overview of its role, objectives, and key activities. Offered to provide more details if needed.\"\n",
      "}\n",
      "</save_recall_memory>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream({\"messages\": [(\"user\", \"What is WHO\")]}, config=config):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\n\\n</think>\\n\\nI only remember what we discuss in our current chat. Each conversation starts fresh to better protect user privacy. Feel free to tell me any preferences or topics you'd like me to focus on - I'll adjust accordingly.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 8, 'total_tokens': 56, 'completion_time': 0.174545455, 'prompt_time': 0.003604036, 'queue_time': 0.047140473, 'total_time': 0.178149491}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_fc872c073e', 'finish_reason': 'stop', 'logprobs': None}, id='run-0e6cbe36-329a-4eb0-9f3b-11290d7d05d2-0', usage_metadata={'input_tokens': 8, 'output_tokens': 48, 'total_tokens': 56})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nHello! How can I assist you today? ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 6, 'total_tokens': 22, 'completion_time': 0.058181818, 'prompt_time': 0.003487132, 'queue_time': 0.046382137000000004, 'total_time': 0.06166895}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_2834edf0f4', 'finish_reason': 'stop', 'logprobs': None}, id='run-cc49ecb5-cf80-4194-8f23-5be3feb6d74e-0', usage_metadata={'input_tokens': 6, 'output_tokens': 16, 'total_tokens': 22})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()\n",
    "\n",
    "# Get text content\n",
    "text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 15:32:32 - USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader('https://www.geeksforgeeks.org/machine-learning/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_data = loader.load()\n",
    "page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input\n"
     ]
    }
   ],
   "source": [
    "user_input =\"http://jks\"\n",
    "if user_input.startswith(\"http://\") or user_input.startswith(\"https://\"):\n",
    "    print(\"Invalid input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: malformed \\N character escape (413114150.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    a = [File(thread_id='78581cd1-df3a-4346-aeaf-30694852b3b1', name='s00466-022-02252-0.pdf', id='e3dab057-e37d-4927-8b1a-fc9ead9643c8', chainlit_key='e3dab057-e37d-4927-8b1a-fc9ead9643c8', url=None, object_key=None, path='G:\\NLP\\hugging_face_model\\.files\\edfd1403-1c0d-448a-b814-716e1fe47b9a\\e3dab057-e37d-4927-8b1a-fc9ead9643c8.pdf', content=None, display='inline', size=None, for_id='597f2ab1-f578-4675-906a-c79b168fd2a6', language=None, mime='pdf')]\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                                                                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: malformed \\N character escape\n"
     ]
    }
   ],
   "source": [
    "a = [File(thread_id='78581cd1-df3a-4346-aeaf-30694852b3b1', name='s00466-022-02252-0.pdf', id='e3dab057-e37d-4927-8b1a-fc9ead9643c8', chainlit_key='e3dab057-e37d-4927-8b1a-fc9ead9643c8', url=None, object_key=None, path='G:\\NLP\\hugging_face_model\\.files\\edfd1403-1c0d-448a-b814-716e1fe47b9a\\e3dab057-e37d-4927-8b1a-fc9ead9643c8.pdf', content=None, display='inline', size=None, for_id='597f2ab1-f578-4675-906a-c79b168fd2a6', language=None, mime='pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
